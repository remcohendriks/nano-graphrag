# Configuration for LMStudio mode
# Uses local models via OpenAI-compatible API

# LLM Configuration
LLM_PROVIDER=openai
LLM_MODEL=qwen3-30b-a3b-instruct-2507
LLM_MAX_TOKENS=24000
LLM_MAX_CONCURRENT=1
LLM_CACHE_ENABLED=true
LLM_TEMPERATURE=0.0

# LMStudio endpoint
OPENAI_BASE_URL=http://192.168.1.5:9090/v1

# Embedding Configuration (still uses OpenAI)
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSION=1536
EMBEDDING_BATCH_SIZE=32
EMBEDDING_MAX_CONCURRENT=8

# Storage Configuration
STORAGE_VECTOR_BACKEND=nano
STORAGE_GRAPH_BACKEND=networkx
STORAGE_KV_BACKEND=json
# Working dir set dynamically by health check script

# Chunking Configuration (tuned for speed)
CHUNKING_STRATEGY=token
# Larger chunks to reduce number of LLM calls
CHUNKING_SIZE=1200
# Standard overlap
CHUNKING_OVERLAP=100
CHUNKING_TOKENIZER=tiktoken
CHUNKING_TOKENIZER_MODEL=gpt-4o

# Entity Extraction (tuned for speed)
# No gleaning for fastest speed
ENTITY_MAX_GLEANING=0
# Shorter summaries
ENTITY_SUMMARY_MAX_TOKENS=300
ENTITY_STRATEGY=llm

# Graph Clustering (tuned for speed)
GRAPH_CLUSTERING_ALGORITHM=leiden
# Reasonable cluster size for better community detection
GRAPH_MAX_CLUSTER_SIZE=10
GRAPH_CLUSTERING_SEED=3967219502

# Query Configuration
QUERY_ENABLE_LOCAL=true
QUERY_ENABLE_GLOBAL=true
QUERY_ENABLE_NAIVE_RAG=true
QUERY_SIMILARITY_THRESHOLD=0.2